{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f0aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddcebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_cleaned.csv')\n",
    "test = pd.read_csv('test_cleaned.csv')\n",
    "#df.drop(['id'],axis=1, inplace=True)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b5423",
   "metadata": {},
   "source": [
    "# Labels are as follows:\n",
    "label '1' ---> racist/sexist tweet           \n",
    "label '0' ---> not racist/sexist tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7d62f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (29530, 3)\n",
      "Test shape:  (17197, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking Shape of Train and Test sets:\n",
    "print(\"Train shape: \", df.shape)\n",
    "print(\"Test shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd854e",
   "metadata": {},
   "source": [
    "# Cleaning data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d82b50",
   "metadata": {},
   "source": [
    "### Cleaning and removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7585b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521d5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations_list = string.punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df['tweet'] = df['tweet'].astype(str)\n",
    "df['tweet'] = df['tweet'].apply(lambda x: cleaning_punctuations(x))\n",
    "\n",
    "test['tweet'] = test['tweet'].astype(str)\n",
    "test['tweet'] = test['tweet'].apply(lambda x: cleaning_punctuations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f929337",
   "metadata": {},
   "source": [
    "### Removing Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca41a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b4a14",
   "metadata": {},
   "source": [
    "### Removing Numeric numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1980b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda text: cleaning_numbers(text))\n",
    "test['tweet'] = test['tweet'].apply(lambda text: cleaning_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428fc42f",
   "metadata": {},
   "source": [
    "### Tokenizing Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cce2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = (word_tokenize(i) for i in df.tweet)\n",
    "df['tweet'] = df['tweet'].apply(nltk.word_tokenize)\n",
    "\n",
    "tokens = (word_tokenize(i) for i in test.tweet)\n",
    "test['tweet'] = test['tweet'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef1929",
   "metadata": {},
   "source": [
    "### Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6de99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemm = SnowballStemmer('english')\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x: [stemm.stem(y) for y in x])\n",
    "test['tweet'] = test['tweet'].apply(lambda x: [stemm.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe3090",
   "metadata": {},
   "source": [
    "### Splitting data into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b971a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not spliiting, Creating X_train and y_train.\n",
    "# Using 100% data for training SVC model to get better training. Because from Step - 2,\n",
    "# it can be concluded that SVC model with 'TF-IDF Vectorizer (1,2) - unigrams and bigrams' performs best for this dataset\n",
    "\n",
    "\n",
    "X_train = df['tweet'].astype(str)    # Converting to string, because vectorizer does'nt accept list.\n",
    "y_train = df['label'].astype(str)    # Converting to string, because vectorizer does'nt accept list.\n",
    "X_test = test['tweet'].astype(str)   # Converting to string, because vectorizer does'nt accept list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c80d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29530,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f5e5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29530,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285b1605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17197,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbeb52",
   "metadata": {},
   "source": [
    "# Transforming Dataset using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b9e20",
   "metadata": {},
   "source": [
    "### Fitting the Count Vectorizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f1a5036",
   "metadata": {},
   "source": [
    "# Extracting features using CountVectorizer - unigrams\n",
    "vectoriser = CountVectorizer(ngram_range=(1,2))\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa39ec",
   "metadata": {},
   "source": [
    "### Fitting the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ebd4d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature_words:  186052\n"
     ]
    }
   ],
   "source": [
    "# Extracting features using TF-IDF (1,2) - unigrams and bigrams\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff38332",
   "metadata": {},
   "source": [
    "### Transforming the data using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb2880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc344c5",
   "metadata": {},
   "source": [
    "# SVC Model since this model performed best during experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eef78590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "hyperParam = {'C': [0.1, 1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001],'kernel': ['rbf','linear','poly','sigmoid']}\n",
    "\n",
    "gsv = GridSearchCV(svc,hyperParam,cv=5,verbose=1,n_jobs=-1)  # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n",
    "best_model = gsv.fit(X_train, y_train)                       # Training model with X_train and y_train\n",
    "svc_pred = best_model.predict(X_test)                        # Predicting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3acc8609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HyperParameter:  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best HyperParameter: \", gsv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34481a",
   "metadata": {},
   "source": [
    "### Creating Submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ac3bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '0' ... '0' '0' '0']\n",
      "<class 'numpy.ndarray'>\n",
      "17197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>49155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17193</th>\n",
       "      <td>49156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17194</th>\n",
       "      <td>49157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17195</th>\n",
       "      <td>49158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17196</th>\n",
       "      <td>49159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17197 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id label\n",
       "0      31963     0\n",
       "1      31964     1\n",
       "2      31965     0\n",
       "3      31966     0\n",
       "4      31967     0\n",
       "...      ...   ...\n",
       "17192  49155     1\n",
       "17193  49156     0\n",
       "17194  49157     0\n",
       "17195  49158     0\n",
       "17196  49159     0\n",
       "\n",
       "[17197 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(svc_pred)\n",
    "print(type(svc_pred))\n",
    "\n",
    "my_array = svc_pred\n",
    "print(len(my_array))\n",
    "\n",
    "submission = pd.DataFrame(my_array,columns = ['label'])\n",
    "submission['id'] = test['id']\n",
    "submission = submission[['id','label']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59414efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
